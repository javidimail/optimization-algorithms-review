{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "706f4146-6e56-40a9-8338-7db8abf23bf9",
        "_uuid": "15c9c8fc01a2a5e24b77d54f23847e55a1a569bd"
      },
      "cell_type": "markdown",
      "source": "# Optimization Algorithms: A Review\n__Reza Javidi__, _PhD Candidate in Computational Statistical Mechanics_\n\n_[This kerenel is under cunstruction]_\n\nDuring my PhD years I had to use a lot of optimization algorithms for my computational research. Here, I share a summary of what I have used. I will keep adding algorithms to this kernel one by one. "
    },
    {
      "metadata": {
        "_cell_guid": "4becbe5c-c499-4726-9589-e59e44a88fdf",
        "_uuid": "52dc0d2dd1e2f0d4b59c1d14cbeeccc8c2226df3"
      },
      "cell_type": "markdown",
      "source": "## 1. Gradient Descent\nOne of the oldest algorithms to find the minimum value of a differentiable function, $f(x)$. __Gradient__ and __Descent__ tell you that this method uses the __gradient__ of the function and __moves downward iteratively__ toward the minimum.\n\n\n$$\nx_{i+1}=x_i-\\gamma \\frac{\\partial f}{\\partial x_{i}},\\ i \\ge 0\n$$\nWhere $\\gamma$ is the learning rate, or the step size which could be constant during the whole process.\n"
    },
    {
      "metadata": {
        "_cell_guid": "11e7cf35-0477-4005-880b-0c1076325b53",
        "_uuid": "b738078f6c74bf530fabdf102127b6e74cf73ef4"
      },
      "cell_type": "markdown",
      "source": "## 1.1. Python Example"
    },
    {
      "metadata": {
        "_cell_guid": "2e48d632-45f7-4653-94d4-03148e8e0fc8",
        "_kg_hide-output": true,
        "_kg_hide-input": true,
        "_uuid": "e551b35d7d65a0a27016815ba260d840513fe66c",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\nplt.style.use('bmh')\nfont = {'family' : 'sans-serif',\n        'weight' : 'normal',\n        'size'   : 14}\nmpl.rc('font', **font)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "03579163-ff79-4716-9825-5e01131160ca",
        "_uuid": "963b2633b93cd9e0bb0c409d72520d73800a4a73"
      },
      "cell_type": "markdown",
      "source": "### Step 1: Define a function and take the derivative \nLet us define $f(x) = x^4$ and $\\frac{df}{dx}=4x^3$. We know that the min value of $f(x) = x^4$ is located at $x_{min}=0$. We just want to test the algorithm."
    },
    {
      "metadata": {
        "_cell_guid": "f9792c8e-006e-4f29-9e86-573326570e41",
        "_uuid": "88351956e2f44bcc626c83cc0a66f1d6d890751b",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Define a differentiable function and its derivative\nf = lambda x: x**4\ndf = lambda x: 4 * x**3\n\n# Let us plot the f(x)\nfig, axes = plt.subplots(figsize=(3,4))\nx = np.linspace(-4,4,num=100,endpoint=True) # define a range for x\nplt.plot(x,f(x), linewidth=4,zorder=1, color='royalblue')\nplt.xlabel('x')\nplt.ylabel('f(x)')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "67bc9bb0-4f0a-4778-b26c-f3c161f7fbe0",
        "_uuid": "e2e0ec43e0110880b8e57278ed4a9cccafb22ede"
      },
      "cell_type": "markdown",
      "source": "### Step 2. Perform the first iteration manually\nIn fthe irst iteration $i=1$ and by setting the $\\gamma = 0.01$, and $x_0=2$ we can calculate $x_1$:\n$$\nx_{i+1}=x_i-\\gamma (\\frac{\\partial f}{\\partial x_{i}}),\\ i \\ge 0\n$$\n$$\nx_{1}=2-0.01\\times(4\\times(2)^3)\\\n$$\n$$\nx_{1}=1.68\n$$\n$$\nx_{1}-x_{0}=1.68 -2 = -0.32\n$$ \nNow we put $x_1 = 1.68$ we should do the a new iteration to find $x_2$ in iteration $i=1$"
    },
    {
      "metadata": {
        "_cell_guid": "4ab07c8c-0df9-4906-80b3-f178f51e2c1b",
        "_uuid": "e10d738edbda5552046419b88edf01383f9bcfaf"
      },
      "cell_type": "markdown",
      "source": "### Step 3. Now let the computer do all the iterations for you"
    },
    {
      "metadata": {
        "_cell_guid": "24db29de-1e75-4bb9-9126-32e932fd8a15",
        "_kg_hide-output": false,
        "scrolled": false,
        "_uuid": "9e691d89d06f0df6c9ee7065fe9e3ae11b25bd62",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "xi = 2 # Initial guess\ndiff_xi_xi_plus1 = xi\n\ngamma = 0.01 # Step size multiplier\ndelta_x = 0.02 #Precision\n\nfig, axes = plt.subplots(figsize=(6,8))\nn = 20\ncolors = mpl.cm.autumn(np.linspace(0,1,n))\n\n# Let us plot the f(x)\nx = np.linspace(-5,5,num=100,endpoint=True) # define a range for x\nplt.plot(x,f(x), linewidth=4,zorder=1, color='royalblue') # plot f(x)\n\n#---------------------------------------------------------\n# Start the iteration and keep plotting during the process\n#---------------------------------------------------------\ni = 1\nwhile diff_xi_xi_plus1 > delta_x:\n    # plot the gradient point\n    plt.scatter(xi, f(xi), color='k', s=100, zorder=2)\n    plt.scatter(xi, f(xi), color='white', s=10, zorder=3)\n    \n    # plot the gradient line; (x,y) for y=m(x-x0)+y0\n    df_line_xrange = np.linspace(xi-3,xi+3,5)\n    plt.plot(df_line_xrange, (df(xi) * (df_line_xrange - xi)) + f(xi), color=colors[i], zorder=1)\n\n    if i == 1: xi_plus1 = xi\n    xi_plus1 += -gamma * df(xi)\n    diff_xi_xi_plus1 = abs(xi - xi_plus1)\n    print(\"%d- xi: %f, xi+1: %f, diff_xi_xi_plus1: %f\" % (i, xi,xi_plus1,diff_xi_xi_plus1))\n    xi = xi_plus1\n    i += 1\n\nplt.ylim([0,20])\nplt.xlim([-3,3])\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title('Finding the Minimum of a Differentiable Function \\n by Gradient Descent')\n\nprint(\"Local Minimum: %f with threshold of: %f\" % (xi, delta_x))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5f7e5c09-79d7-4790-a9ac-d0a5b201c3e0",
        "_uuid": "ec78d426873422c50e3edf793ffe3dccaa6c5e0e"
      },
      "cell_type": "markdown",
      "source": "As you can see the algorithm does a good job when initial value is far from the minimum (by taking big steps). Once it gets closer to the minimum value, step size values start decreasing which is not a good thing!"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}